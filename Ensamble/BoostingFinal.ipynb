{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BoostingFinal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "sLGkZoob5l78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "78C7AvDoFVTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "from models import *\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
        "from keras.layers import Input, Dropout, Convolution1D, MaxPool1D, UpSampling1D, concatenate, GlobalMaxPool1D\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "2xl9i_DY5thS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up GPU"
      ],
      "metadata": {
        "id": "6lTRP7eOFYVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != \"/device:GPU:0\":\n",
        "  device_name = \"/cpu:0\"\n",
        "print('Found device at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "60SuH2uQ5j7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Folder Structure"
      ],
      "metadata": {
        "id": "r0vMgt42FfMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = Path(\"../input/\")\n",
        "model_dir = Path(\".\")"
      ],
      "metadata": {
        "id": "cdMgIE5795x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "I_x3NmRV54pO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "format_data is a utility function for relaibly loading and lightly formatting the heartbeat data signals. It can add padding to ensure that signals have a certain lenght."
      ],
      "metadata": {
        "id": "rZvUjple6CaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  :param df: Dataframe containing signal and labels\n",
        "  :param padded_size: Integer indicating if signal should be padded\n",
        "                      to certain length\n",
        "  :return: Signal, Labels\n",
        "\"\"\"\n",
        "def format_data(\n",
        "    df : pd.DataFrame,\n",
        "    padded_size : Optional[int] = None\n",
        ") -> Tuple[np.array, np.array]:\n",
        "\n",
        "    # Load signal and labels from the dataframe\n",
        "    Y = np.array(df[187].values).astype(np.int8)\n",
        "    X = np.array(df[list(range(187))].values)[..., np.newaxis]\n",
        "\n",
        "    # Add padding if padded_size is specified\n",
        "    if not padded_size is None:\n",
        "        X = np.concatenate([X, np.zeros((X.shape[0], padded_size - X.shape[1], 1))], axis=1)\n",
        "\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "4EKp0_Cw5szg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PTB Dataset"
      ],
      "metadata": {
        "id": "3xUKbLgn-UZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "WLAgRi9JEwca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the data using the previously defined utility functions"
      ],
      "metadata": {
        "id": "n-OPYzJFrrVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem parameters\n",
        "unpadded_size = 187\n",
        "padded_size = 256\n",
        "\n",
        "# Load data PTB\n",
        "df_1 = pd.read_csv(data_dir.joinpath(\"ptbdb_normal.csv\"), header=None)\n",
        "df_2 = pd.read_csv(data_dir.joinpath(\"ptbdb_abnormal.csv\"), header=None)\n",
        "df   = pd.concat([df_1, df_2])\n",
        "\n",
        "df_train, df_test = train_test_split(\n",
        "    df, test_size=0.2, \n",
        "    random_state=1337, stratify=df[unpadded_size]\n",
        ")\n",
        "\n",
        "# Format data\n",
        "X_test, Y_test   = format_data(df_test)\n",
        "X_train, Y_train = format_data(df_train)\n"
      ],
      "metadata": {
        "id": "whbwoD5z9161"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "tVbwJW-HyML6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first perform a grid search and then we extract the top-performing model and use this to get our results"
      ],
      "metadata": {
        "id": "OWetNG838srA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters to test\n",
        "parameters = {\n",
        "    \"classes\": [1],\n",
        "    \"n_estimators\":[50, 100],\n",
        "    \"n_filters\": [32, 64, 128], \n",
        "    \"n_dense\": [16], \n",
        "    \"kernel_size\": [5, 8]\n",
        "}\n",
        "\n",
        "# Run CV and predict\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Initializing base learner\n",
        "    base = BoostingCNN(classes = 1, n_estimators = 1)\n",
        "\n",
        "    # Running grid search\n",
        "    search = GridSearchCV(base, parameters, verbose=3, cv=3)\n",
        "    search.fit(X_train, Y_train)\n",
        "    print(f\"Finished CV for PTB Dataset: Top score {search.best_score_}\\n\"\n",
        "              f\"Best parameters: {search.cv_results_['params'][search.best_index_]}\")\n",
        "    \n",
        "    # Run tests\n",
        "    pred_test = search.best_estimator_.predict(X_test)\n",
        "\n",
        "    f1 = f1_score(Y_test, pred_test)\n",
        "    print(\"Test f1 score : %s \"% f1)\n",
        "\n",
        "    acc = accuracy_score(Y_test, pred_test)\n",
        "    print(\"Test accuracy score : %s \"% acc)\n",
        "\n",
        "    auroc = roc_auc_score(Y_test, pred_test)\n",
        "    print(\"Test AUROC : %s \"% auroc)\n",
        "\n",
        "    auprc = average_precision_score(Y_test, pred_test)\n",
        "    print(\"Test AUPRC : %s \"% auprc)\n"
      ],
      "metadata": {
        "id": "9ULF5PWC6Yft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIT-BIH Dataset"
      ],
      "metadata": {
        "id": "z6708qbh-qOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "ZZB5Dd6O-qOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the data using the previously defined utility functions\\\n",
        "In order to make to combat both class imbalance and exploding runtimes, we down-sample the majority class and upsample the minority classes for the training set as described in the report."
      ],
      "metadata": {
        "id": "dp8osLej-qOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem parameters\n",
        "unpadded_size = 187\n",
        "padded_size = 256\n",
        "\n",
        "# Load data MIT\n",
        "df_train = pd.read_csv(data_dir.joinpath(\"mitbih_train.csv\"), header=None)\n",
        "df_train = df_train.sample(frac=1)\n",
        "df_test = pd.read_csv(data_dir.joinpath(\"mitbih_test.csv\"), header=None)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority_size = 8000\n",
        "minority_size = 2000\n",
        " \n",
        "# Downsample majority class\n",
        "df_majority = resample(df_train.loc[df_train[187] == 0], \n",
        "                       replace=False,    \n",
        "                       n_samples=majority_size)\n",
        "\n",
        "# Upsample minority class\n",
        "df_minority = [resample(df_train.loc[df_train[187] == i], \n",
        "                        replace=True,     \n",
        "                        n_samples=minority_size)\n",
        "               for i in range(1, 5)]\n",
        "\n",
        "# Combine minority class with downsampled majority class\n",
        "df_up_down_sampled = pd.concat([df_majority] + df_minority)\n",
        "\n",
        "# Format data\n",
        "X_test, Y_test   = format_data(df_test)\n",
        "X_train, Y_train = format_data(df_up_down_sampled)\n"
      ],
      "metadata": {
        "id": "xLTFB1Z1-qOJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "6ioqRcu2-qOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first perform a grid search and then we extract the top-performing model and use this to get our results"
      ],
      "metadata": {
        "id": "AWYX9SAb-qOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters to test\n",
        "parameters = {\n",
        "    \"classes\": [5],\n",
        "    \"n_estimators\":[50, 100],\n",
        "    \"n_filters\": [32, 64, 128], \n",
        "    \"n_dense\": [16], \n",
        "    \"kernel_size\": [5, 8]\n",
        "}\n",
        "\n",
        "# Run CV and predict\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Initializing base learner\n",
        "    base = BoostingCNN(classes = 5, n_estimators = 1)\n",
        "\n",
        "    # Running grid search\n",
        "    search = GridSearchCV(base, parameters, verbose=3, cv=3)\n",
        "    search.fit(X_train, Y_train)\n",
        "    print(f\"Finished CV for PTB Dataset: Top score {search.best_score_}\\n\"\n",
        "          f\"Best parameters: {search.cv_results_['params'][search.best_index_]}\")\n",
        "    \n",
        "    # Run tests\n",
        "    pred_test = search.best_estimator_.predict(X_test)\n",
        "\n",
        "    f1 = f1_score(Y_test, pred_test, average=\"macro\")\n",
        "    print(\"Test f1 score : %s \"% f1)\n",
        "\n",
        "    acc = accuracy_score(Y_test, pred_test)\n",
        "    print(\"Test accuracy score : %s \"% acc)\n"
      ],
      "metadata": {
        "id": "Maoa6hRt-qOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fjX5d-mkBr0d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}