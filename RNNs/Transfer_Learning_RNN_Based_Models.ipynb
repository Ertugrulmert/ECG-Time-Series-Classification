{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca36644d",
   "metadata": {},
   "source": [
    "# Task 4: RNN Based Transfer Learning \n",
    "--------------------------------------------------\n",
    "This notebook is comprised of the transfer learning (Task 4) part of the project. Two best performing RNN based models, which are Bidirectional LSTM and ConvLSTM, are selected and both models are trained on the larger MIT-BIH dataset. Then, their final fully connected layer is changed to convert them into binary classifiers intended to work for the smaller PTB dataset. Two approaches of transfer learning are applied to both models. \n",
    "\n",
    "- For the first approach, the pre-trained weights are leveraged without freezing any layers so the entire model is tuned again by training on the PTB dataset.\n",
    "\n",
    "- In the second approach, most of the model layers are kept frozen and only the fully connected layers at the end are tuned on the PTB dataset.  \n",
    "\n",
    "- At the bottom of the notebook, for both approaches and both model types, a 5-fold cross validation grid search is carried out to identify the best performing hyperparameter configuration. In the cells at the beginning of the notebook, corresponding optimal hyperparameters are used.\n",
    "\n",
    "\n",
    "# Results\n",
    "\n",
    "### ConvLSTM model\n",
    "#### Without Layer Freezing:\n",
    "\n",
    "Test f1 score : 0.9928571428571429 \n",
    "\n",
    "Test accuracy score : 0.9896942631398145 \n",
    "\n",
    "Test AUROC score : 0.9879216215294399 \n",
    "\n",
    "Test AUPRC score : 0.9916061177955566\n",
    "\n",
    "#### With Layer Freezing:\n",
    "Test f1 score : 0.9025641025641027 \n",
    "\n",
    "Test accuracy score : 0.8564067330814154 \n",
    "\n",
    "Test AUROC score : 0.8047659595487964 \n",
    "\n",
    "Test AUPRC score : 0.8719745378050834 \n",
    "\n",
    "----------------------------------------------------------------\n",
    "\n",
    "### Bidirectional LSTM model\n",
    "#### Without Layer Freezing:\n",
    "\n",
    "Test f1 score : 0.9762357414448669 \n",
    "\n",
    "Test accuracy score : 0.9656475437993817 \n",
    "\n",
    "Test AUROC score : 0.9564438600473502 \n",
    "\n",
    "Test AUPRC score : 0.969526262999743 \n",
    "\n",
    "\n",
    "#### With Layer Freezing:\n",
    "\n",
    "Test f1 score : 0.9054986907879077 \n",
    "\n",
    "Test accuracy score : 0.8636207488835451 \n",
    "\n",
    "Test AUROC score : 0.8306710073048331 \n",
    "\n",
    "Test AUPRC score : 0.8886332095514813\n",
    "\n",
    "### Overall, transfer learning without layer freezing performed better for both models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89100922",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras import losses\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "import model_helper\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from models import *\n",
    "\n",
    "\n",
    "# To ensure reproducable results: \n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc132a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking for GPU for speed-up\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != \"/device:GPU:0\":\n",
    "  device_name = \"/cpu:0\"\n",
    "print('Found device at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8987afc9",
   "metadata": {},
   "source": [
    "# MIT-BIH Arryhtmia Database\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386404d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/mitbih_train.csv\", header=None)\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_test = pd.read_csv(\"../input/mitbih_test.csv\", header=None)\n",
    "\n",
    "Y_mitbih = np.array(df_train[187].values).astype(np.int8)\n",
    "X_mitbih = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test_mitbih = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test_mitbih = np.array(df_test[list(range(187))].values)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1cd6b",
   "metadata": {},
   "source": [
    "# PTB Diagonstic ECG Database\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7befcc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"../input/ptbdb_normal.csv\", header=None)\n",
    "df_2 = pd.read_csv(\"../input/ptbdb_abnormal.csv\", header=None)\n",
    "df = pd.concat([df_1, df_2])\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1337, stratify=df[187])\n",
    "\n",
    "\n",
    "Y_ptbdb = np.array(df_train[187].values).astype(np.int8)\n",
    "X_ptbdb = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test_ptbdb = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test_ptbdb = np.array(df_test[list(range(187))].values)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4b62e",
   "metadata": {},
   "source": [
    "# Transfer Learning for ConvLSTM model\n",
    "------------------------------------------------------------\n",
    "## Without Freezing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_file = True\n",
    "file_path = \"Results/final_cnn_lstm_mitbih.h5\"\n",
    "new_save_path = \"Results/transfer_cnn_lstm.h5\"\n",
    "\n",
    "#first loading weights of / training the ConvLSTM for the larger (MITBIH) dataset\n",
    "\n",
    "if from_file:\n",
    "    model = keras.models.load_model(file_path)\n",
    "    print(\"Trained model weights loaded.\")\n",
    "    \n",
    "# if no save file is available, trainin on MIT dataset first\n",
    "else:\n",
    "    \n",
    "    print(\"Training on MIT Dataset:\")\n",
    "    \n",
    "    with tf.device(device_name):\n",
    "    \n",
    "        #callbacks to stop or change learning rate when held out validation set loss \n",
    "        #stops improving, patience selected high due to instability of RNNs\n",
    "        early = EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1)\n",
    "        redonplat = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, verbose=1)\n",
    "        if file_path:\n",
    "            checkpoint = ModelCheckpoint(filepath=file_path, monitor='val_loss', verbose=1, save_best_only=True) \n",
    "            callbacks_list = [checkpoint, early, redonplat] \n",
    "        else:\n",
    "            callbacks_list = [early, redonplat] \n",
    "    \n",
    "        #creating and trainin model\n",
    "        model = ConvLSTM( input_length=X_ptbdb.shape[1], num_units=150, num_conv=2, num_dense = 2,\n",
    "                           num_classes=2, dropout=0.5, optimizer=\"adam\",callbacks= callbacks_list,  lr=0.001)\n",
    "\n",
    "        model = model_helper.train_test_model( model, X_mitbih, Y_mitbih, X_test_mitbih, Y_test_mitbih,\n",
    "                                     binary_task=False)\n",
    "    \n",
    "    print(\"Training over.\")\n",
    "    \n",
    "print(\"Proceeding to transfer learning on PTB Dataset:\")\n",
    "\n",
    "#adding the new final fully connected layer\n",
    "if isinstance(model, tf.keras.Sequential):\n",
    "    new_model= keras.models.Sequential(model.layers[:-1])\n",
    "    new_model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\"))\n",
    "else:\n",
    "    new_model= keras.models.Sequential(model.model().layers[:-1])\n",
    "    new_model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\")) \n",
    "\n",
    "        \n",
    "with tf.device(device_name):\n",
    "            \n",
    "    # using lower learning rate to allow for more careful fine tuning\n",
    "    new_model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                          metrics=['accuracy', keras.metrics.AUC()])\n",
    "\n",
    "    #fine tuning on PTB dataset\n",
    "    transfer_model = model_helper.train_test_model( new_model, X_ptbdb, Y_ptbdb, X_test_ptbdb, Y_test_ptbdb, epochs=50,\n",
    "                                                     binary_task=True, save_name = new_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444f9d1",
   "metadata": {},
   "source": [
    "## With Freezing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_file = True\n",
    "file_path = \"Results/final_cnn_lstm_mitbih.h5\"\n",
    "new_save_path = \"Results/transfer_cnn_lstm_frozen.h5\"\n",
    "\n",
    "#first loading weights of / training the ConvLSTM for the larger (MITBIH) dataset\n",
    "\n",
    "if from_file:\n",
    "    model = keras.models.load_model(file_path)\n",
    "    print(\"Trained model weights loaded.\")\n",
    "# if no save file is available, trainin on MIT dataset first   \n",
    "else:\n",
    "    \n",
    "    print(\"Training on MIT Dataset:\")\n",
    "    \n",
    "    with tf.device(device_name):\n",
    "    \n",
    "        #callbacks to stop or change learning rate when held out validation set loss \n",
    "        #stops improving, patience selected high due to instability of RNNs\n",
    "        early = EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1)\n",
    "        redonplat = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, verbose=1)\n",
    "        if save_name:\n",
    "            checkpoint = ModelCheckpoint(filepath=save_name, monitor='val_loss', verbose=1, save_best_only=True) \n",
    "            callbacks_list = [checkpoint, early, redonplat] \n",
    "        else:\n",
    "            callbacks_list = [early, redonplat] \n",
    "    \n",
    "        #creating and trainin model\n",
    "        model = ConvLSTM( input_length=X_ptbdb.shape[1], num_units=150, num_conv=2, num_dense = 2,\n",
    "                          classes=2, dropout=0.5, optimizer=\"adam\", callbacks= callbacks_list,lr=0.001)\n",
    "\n",
    "        model = model_helper.train_test_model( model, X_mitbih, Y_mitbih, X_test_mitbih, Y_test_mitbih,\n",
    "                                     binary_task=False)\n",
    "    \n",
    "    print(\"Training over.\")\n",
    "    \n",
    "print(\"Proceeding to transfer learning on PTB Dataset:\")\n",
    "\n",
    "#adding the new final fully connected layer\n",
    "if isinstance(model, tf.keras.Sequential):\n",
    "    new_model= keras.models.Sequential(model.layers[:-1])\n",
    "    new_model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\"))\n",
    "else:\n",
    "    new_model= keras.models.Sequential(model.model().layers[:-1])\n",
    "    new_model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\")) \n",
    "\n",
    "#freezing the layers before the final 2 fully connected layers\n",
    "for layer in new_model.layers[:-2]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "with tf.device(device_name):\n",
    "    \n",
    "    # using lower learning rate to allow for more careful fine tuning\n",
    "    new_model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                          metrics=['accuracy', keras.metrics.AUC()])\n",
    "\n",
    "    #fine tuning on PTB dataset\n",
    "    transfer_model = model_helper.train_test_model( new_model, X_ptbdb, Y_ptbdb, X_test_ptbdb, Y_test_ptbdb, epochs=100,\n",
    "                                                     binary_task=True, save_name = new_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf2390",
   "metadata": {},
   "source": [
    "# Transfer Learning for Bidirectional LSTM model\n",
    "------------------------------------------------------------\n",
    "## Without Freezing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3aca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_file = True\n",
    "file_path = \"Results/final_bdlstm_mitbih.h5\"\n",
    "new_save_path = \"Results/transfer_bdlstm.h5\"\n",
    "\n",
    "#first loading weights of / training the Bidirectional LSTM for the larger (MITBIH) dataset\n",
    "\n",
    "if from_file:\n",
    "    model = keras.models.load_model(file_path)\n",
    "    print(\"Trained model weights loaded.\")\n",
    "# if no save file is available, trainin on MIT dataset first    \n",
    "else:\n",
    "    \n",
    "    print(\"Training on MIT Dataset:\")\n",
    "    \n",
    "    with tf.device(device_name):\n",
    "    \n",
    "        #callbacks to stop or change learning rate when held out validation set loss \n",
    "        #stops improving, patience selected high due to instability of RNNs\n",
    "        early = EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1)\n",
    "        redonplat = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, verbose=1)\n",
    "        if save_name:\n",
    "            checkpoint = ModelCheckpoint(filepath=save_name, monitor='val_loss', verbose=1, save_best_only=True) \n",
    "            callbacks_list = [checkpoint, early, redonplat] \n",
    "        else:\n",
    "            callbacks_list = [early, redonplat] \n",
    "    \n",
    "        #creating and trainin model\n",
    "        model = BiDirLSTM( input_length=X_mitbih.shape[1], num_units=100, classes=5, num_cells = 2, \n",
    "                        num_dense = 2, dropout=0, optimizer=\"adam\", callbacks= callbacks_list,lr=0.0001)\n",
    "\n",
    "        model = model_helper.train_test_model( model, X_mitbih, Y_mitbih, X_test_mitbih, Y_test_mitbih,\n",
    "                                     binary_task=False)\n",
    "    \n",
    "    print(\"Training over.\")\n",
    "    \n",
    "print(\"Proceeding to transfer learning on PTB Dataset:\")\n",
    "\n",
    "#adding the new final fully connected layer\n",
    "if isinstance(model, tf.keras.Sequential):\n",
    "    new_model= keras.models.Sequential(model.layers[:-1])\n",
    "    new_model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\"))\n",
    "else:\n",
    "    new_model= keras.models.Sequential(model.model().layers[:-1])\n",
    "    new_model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\")) \n",
    "\n",
    "\n",
    "        \n",
    "with tf.device(device_name):\n",
    "    \n",
    "    # using lower learning rate to allow for more careful fine tuning\n",
    "    new_model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0001), \n",
    "                          metrics=['accuracy', keras.metrics.AUC()])\n",
    "\n",
    "    #fine tuning on PTB dataset\n",
    "    transfer_model = model_helper.train_test_model( new_model, X_ptbdb, Y_ptbdb, X_test_ptbdb, Y_test_ptbdb, epochs=50,\n",
    "                                                     binary_task=True, save_name = new_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127df0bc",
   "metadata": {},
   "source": [
    "## With Freezing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebe338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_file = True\n",
    "file_path = \"Results/final_bdlstm_mitbih.h5\"\n",
    "new_save_path = \"Results/transfer_bdlstm_frozen.h5\"\n",
    "\n",
    "#first loading weights of / training the Bidirectional LSTM for the larger (MITBIH) dataset\n",
    "\n",
    "if from_file:\n",
    "    model = keras.models.load_model(file_path)\n",
    "    print(\"Trained model weights loaded.\")\n",
    "# if no save file is available, trainin on MIT dataset first    \n",
    "else:\n",
    "    \n",
    "    print(\"Training on MIT Dataset:\")\n",
    "    \n",
    "    with tf.device(device_name):\n",
    "    \n",
    "        #callbacks to stop or change learning rate when held out validation set loss \n",
    "        #stops improving, patience selected high due to instability of RNNs\n",
    "        early = EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1)\n",
    "        redonplat = ReduceLROnPlateau(monitor=\"val_loss\", patience=7, verbose=1)\n",
    "        if save_name:\n",
    "            checkpoint = ModelCheckpoint(filepath=save_name, monitor='val_loss', verbose=1, save_best_only=True) \n",
    "            callbacks_list = [checkpoint, early, redonplat] \n",
    "        else:\n",
    "            callbacks_list = [early, redonplat] \n",
    "    \n",
    "        #creating and trainin model\n",
    "        model = BiDirLSTM( input_length=X_mitbih.shape[1], num_units=100, classes=5, num_cells = 2, \n",
    "                        num_dense = 2, dropout=0, optimizer=\"adam\", callbacks= callbacks_list, lr=0.0001)\n",
    "\n",
    "        model = model_helper.train_test_model( model, X_mitbih, Y_mitbih, X_test_mitbih, Y_test_mitbih,\n",
    "                                     binary_task=False)\n",
    "    \n",
    "    print(\"Training over.\")\n",
    "    \n",
    "print(\"Proceeding to transfer learning on PTB Dataset:\")\n",
    "\n",
    "#adding the new final fully connected layer\n",
    "if isinstance(model, tf.keras.Sequential):\n",
    "    new_model= keras.models.Sequential(model.layers[:-1])\n",
    "    new_model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\"))\n",
    "else:\n",
    "    new_model= keras.models.Sequential(model.model().layers[:-1])\n",
    "    new_model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\")) \n",
    "\n",
    "#in this version, we will not freeze layers\n",
    "for layer in new_model.layers[:-2]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "with tf.device(device_name):\n",
    "    \n",
    "    # using lower learning rate to allow for more careful fine tuning\n",
    "    new_model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0001), \n",
    "                          metrics=['accuracy', keras.metrics.AUC()])\n",
    "\n",
    "    #fine tuning on PTB dataset\n",
    "    transfer_model = model_helper.train_test_model( new_model, X_ptbdb, Y_ptbdb, X_test_ptbdb, Y_test_ptbdb, epochs=100,\n",
    "                                                     binary_task=True, save_name = new_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70b0de",
   "metadata": {},
   "source": [
    "## Hyperparameter Search for Transfer Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed2681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_grid_search( model = \"bdlstm\", frozen=True):\n",
    "    \"\"\"Performs hyperparameter grid search with 5-fold corss validation\n",
    "\n",
    "    Keyword arguments:\n",
    "    model -- chooses between Bidirectional LSTM (bdlstm) and ConvLSTM (default bdlstm)\n",
    "    frozen -- whether to freeze layers before \n",
    "              the fully conencted layers at the end (default True)\n",
    "    \"\"\"\n",
    "\n",
    "    #optimizer and other unit number options were discarded as grid search takes too long\n",
    "    learning_rates = [0.0001, 0.001, 0.000001]\n",
    "    batch = 200\n",
    "    epochs = 50\n",
    "    optimizers = [\"adam\", \"rmsprop\"]\n",
    "\n",
    "    file_path_cnnlstm = \"Results/final_cnn_lstm_mitbih.h5\"\n",
    "    file_path_bdlstm = \"Results/final_bdlstm_mitbih.h5\"\n",
    "\n",
    "    opt_params = {}\n",
    "    best_AUC = 0\n",
    "    best_acc = 0\n",
    "    scores = []\n",
    "    \n",
    "    (X,Y) = (X_ptbdb, Y_ptbdb) \n",
    "\n",
    "    for optim in optimizers:\n",
    "        for lr in learning_rates:\n",
    "                        print(\"---------------------------------------------------\")\n",
    "                        print(\"Params to evaluate:\")\n",
    "                        print(\"LR: \",lr, \" | Optim: \",optim)\n",
    "\n",
    "                        scores = []\n",
    "\n",
    "                        for train, val in KFold(n_splits=5, shuffle=True).split(X,Y):\n",
    "                            \n",
    "                                print(\"Fold - \", len(scores)+1)\n",
    "\n",
    "                                #callbacks enable early stopping and learning rate reduction\n",
    "                                #depending on validation loss\n",
    "                                early = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=10, verbose=1)\n",
    "                                redonplat = ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", patience=7, verbose=1)\n",
    "                                callbacks_list = [early, redonplat]\n",
    "                                \n",
    "                                with tf.device(device_name):\n",
    "                                    \n",
    "                                \n",
    "                                    if model == \"bdlstm\":\n",
    "                                        model = keras.models.load_model(file_path_bdlstm)\n",
    "                                    else:\n",
    "                                        model = keras.models.load_model(file_path_cnnlstm)\n",
    "\n",
    "                                    print(\"Trained model weights loaded.\")\n",
    "\n",
    "\n",
    "                                    #adding the new final fully connected layer\n",
    "                                    if isinstance(model, tf.keras.Sequential):\n",
    "                                        new_model= keras.models.Sequential(model.layers[:-1])\n",
    "                                        new_model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\"))\n",
    "                                    else:\n",
    "                                        new_model= keras.models.Sequential(model.model().layers[:-1])\n",
    "                                        new_model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\")) \n",
    "\n",
    "                                    #freezing the layers before the final 2 fully connected layers\n",
    "                                    if frozen:\n",
    "                                        for layer in new_model.layers[:-2]:\n",
    "                                                layer.trainable = False\n",
    "\n",
    "                      \n",
    "                                    # using lower learning rate to allow for more careful fine tuning\n",
    "                                    new_model.compile(loss='binary_crossentropy', optimizer=optim,\n",
    "                                                          metrics=['accuracy', keras.metrics.AUC()])\n",
    "\n",
    "                                    # training the model\n",
    "                                    new_model.fit(X[train], Y[train], epochs=epochs, batch_size=batch, \n",
    "                                            verbose=0, callbacks=callbacks_list, validation_data = (X[val],Y[val]) )\n",
    "\n",
    "                                    # evaluate validation set\n",
    "                                    scores.append(new_model.score(X[val],Y[val])) \n",
    "                                \n",
    "                                print(\"Fold Accuracy: \", scores[-1])\n",
    "\n",
    "                        avg_acc = np.asarray(scores).mean()\n",
    "\n",
    "                        print(\"-------------------------- RESULTS -------------------------- \")\n",
    "\n",
    "                        print(\"average 5-fold cross val accuracy: \", avg_acc)\n",
    "\n",
    "                        if avg_acc > best_acc:\n",
    "                            best_acc = avg_acc\n",
    "                            opt_params[\"optim\"] = optim\n",
    "                            opt_params[\"lr\"] = lr\n",
    "\n",
    "                            \n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\" \")\n",
    "    print(\"Best Params:\")\n",
    "    print(\"LR: \",opt_params[\"lr\"], \" | Optim: \", opt_params[\"optim\"])\n",
    "    print(\" \")\n",
    "    print(\"Best Accuracy: \", best_acc)\n",
    "\n",
    "    return opt_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51407fe5",
   "metadata": {},
   "source": [
    "#### Grid Search For Bidirectional LSTM without Layer Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da28b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_grid_search( model = \"bdlstm\", frozen=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7b6c6",
   "metadata": {},
   "source": [
    "#### Grid Search For Bidirectional LSTM with Layer Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55083188",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_grid_search( model = \"bdlstm\", frozen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2193dc",
   "metadata": {},
   "source": [
    "#### Grid Search For ConvLSTM without Layer Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_grid_search( model = \"convlstm\", frozen=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da8dd5",
   "metadata": {},
   "source": [
    "#### Grid Search For ConvLSTM with Layer Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9419306",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_grid_search( model = \"convlstm\", frozen=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
