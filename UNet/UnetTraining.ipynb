{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UnetTraining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "sLGkZoob5l78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "78C7AvDoFVTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "from models import *\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
        "from keras.layers import Input, Dropout, Convolution1D, MaxPool1D, UpSampling1D, concatenate, GlobalMaxPool1D\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "2xl9i_DY5thS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up GPU"
      ],
      "metadata": {
        "id": "6lTRP7eOFYVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != \"/device:GPU:0\":\n",
        "  device_name = \"/cpu:0\"\n",
        "print('Found device at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60SuH2uQ5j7k",
        "outputId": "4752b3f9-7b10-4e18-dc18-52b6ff5590a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found device at: /cpu:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Folder Structure"
      ],
      "metadata": {
        "id": "r0vMgt42FfMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = Path(\"../input/\")\n",
        "model_dir = Path(\".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdMgIE5795x7",
        "outputId": "0eaf8842-a710-492e-c7a7-6f293f03db3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "I_x3NmRV54pO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "format_data is a utility function for relaibly loading and lightly formatting the heartbeat data signals. It can add padding to ensure that signals have a certain lenght."
      ],
      "metadata": {
        "id": "rZvUjple6CaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  :param df: Dataframe containing signal and labels\n",
        "  :param padded_size: Integer indicating if signal should be padded\n",
        "                      to certain length\n",
        "  :return: Signal, Labels\n",
        "\"\"\"\n",
        "def format_data(\n",
        "    df : pd.DataFrame,\n",
        "    padded_size : Optional[int] = None\n",
        ") -> Tuple[np.array, np.array]:\n",
        "\n",
        "    # Load signal and labels from the dataframe\n",
        "    Y = np.array(df[187].values).astype(np.int8)\n",
        "    X = np.array(df[list(range(187))].values)[..., np.newaxis]\n",
        "\n",
        "    # Add padding if padded_size is specified\n",
        "    if not padded_size is None:\n",
        "        X = np.concatenate([X, np.zeros((X.shape[0], padded_size - X.shape[1], 1))], axis=1)\n",
        "\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "4EKp0_Cw5szg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mask is a function to softly mask a heartbeat signal.\\\n",
        "masked_loss is the loss function described in the report.\\\n",
        "We combine the soft mask and the true signal in one 4D tensor to not run into troubles with batching."
      ],
      "metadata": {
        "id": "gxCAQ2mq7E3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  :param X: Signal to be masked\n",
        "  :param unpadded_size: Length of signal before any padding\n",
        "                        Only want to mask parts of the original signal\n",
        "  :param mask_size: Length of patches to mask\n",
        "  :param max_repeats: Maximum number of patches to mask for a single signal\n",
        "  :param alpah: Parameter to construct soft mask\n",
        "  :return: Masked signal, Soft mask\n",
        "\"\"\"\n",
        "def mask(X, unpadded_size, mask_size , max_repeats, alpha):\n",
        "\n",
        "    # All part of signal is initially unmasked\n",
        "    M = np.ones_like(X, dtype=np.float32)\n",
        "\n",
        "    # Randomly mask patches of size mask_size\n",
        "    for i in range(X.shape[0]):\n",
        "        for _ in range(np.random.randint(low=0, high=max_repeats) + 1):\n",
        "            start = np.random.randint(low=0, high=unpadded_size - mask_size)\n",
        "            end = start + mask_size\n",
        "            M[i, start : end] = 0\n",
        "\n",
        "    return M * X, np.stack([X, 1 - alpha * M], axis=-1)\n",
        "\n",
        "\"\"\"\n",
        "    :param stacked_input: 4d Tensor. \n",
        "                          stacked_input[..., 0] is the original unmaksed signal\n",
        "                          stacked_input[..., 1] is the soft mask\n",
        "    :param pred: 3d tensor which is the predicted signal from the model\n",
        "    :return: Softly masked reconstruction loss\n",
        "\"\"\"\n",
        "def masked_loss(stacked_input, pred):\n",
        "    squared_difference = stacked_input[:, : ,: , 1] * tf.square(pred - stacked_input[:, :, :, 0])\n",
        "    return tf.reduce_mean(squared_difference, axis=-1)"
      ],
      "metadata": {
        "id": "QqA6Y3gO7ItT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the data using the previously defined utility functions"
      ],
      "metadata": {
        "id": "bDYzIumJ9hKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Problem parameters\n",
        "unpadded_size = 187\n",
        "padded_size = 256\n",
        "\n",
        "# Masking parameters\n",
        "mask_size = 60\n",
        "max_repeats = 2\n",
        "alpha = 0.95\n",
        "\n",
        "\n",
        "# Load data PTB\n",
        "df_1 = pd.read_csv(data_dir.joinpath(\"ptbdb_normal.csv\"), header=None)\n",
        "df_2 = pd.read_csv(data_dir.joinpath(\"ptbdb_abnormal.csv\"), header=None)\n",
        "df   = pd.concat([df_1, df_2])\n",
        "\n",
        "df_train_ptb, df_test_ptb = train_test_split(\n",
        "    df, test_size=0.2, \n",
        "    random_state=1337, stratify=df[unpadded_size]\n",
        ")\n",
        "\n",
        "# Load data MIT\n",
        "df_train_mit = pd.read_csv(data_dir.joinpath(\"mitbih_train.csv\"), header=None)\n",
        "df_train_mit = df_train_mit.sample(frac=1)\n",
        "df_test_mit = pd.read_csv(data_dir.joinpath(\"mitbih_test.csv\"), header=None)\n",
        "\n",
        "# Combine\n",
        "df_train = pd.concat([df_train_ptb, df_train_mit])\n",
        "df_test = pd.concat([df_test_ptb, df_test_mit])\n",
        "\n",
        "# Format and mask data\n",
        "X_test, Y_test     = format_data(df_test, padded_size)\n",
        "X_train, Y_train   = format_data(df_train, padded_size)\n",
        "X_masked, Y_masked = mask(X_train, unpadded_size, mask_size , max_repeats, alpha)\n"
      ],
      "metadata": {
        "id": "IqzYCGo49pre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "3xUKbLgn-UZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training procedure for the U-Net\\\n",
        "We don't do much hyperparameter tuning due to the imense training time for a single model.\n",
        "We observe that $\\alpha$ is not too high so that the model collapses to the mean signal strength. Otherwise, we use the paramteres described by the authors of the U-Net paper. Moreover, we use a low patience for early stopping and reduction of the learning rate due to the big size of the model and the dataset."
      ],
      "metadata": {
        "id": "LHeeMC3c-uNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up callbacks \n",
        "# Due to the long training time for a single epoch,\n",
        "# we use a low patience for reducing learning rate \n",
        "# and for stopping early\n",
        "unet_file_path = model_dir.joinpath(\"unet.h5\")\n",
        "checkpoint = ModelCheckpoint(unet_file_path, monitor='val_loss', verbose=1, save_best_only=True, mode=\"min\")\n",
        "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, verbose=1)\n",
        "redonplat = ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\", patience=3, verbose=2)\n",
        "callbacks_list = [checkpoint, early, redonplat]\n",
        "\n",
        "# Run training\n",
        "model = Unet(\n",
        "    (padded_size,1), loss = masked_loss,\n",
        "    epochs=100, verbose=2, callbacks=callbacks_list, validation_split=0.1\n",
        ")\n",
        "model.fit(X_masked, Y_masked)\n",
        "model.load_weights(unet_file_path)"
      ],
      "metadata": {
        "id": "whbwoD5z9161"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Models in Latent Space"
      ],
      "metadata": {
        "id": "xpOtJlf6jca9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on PTBDB Dataset"
      ],
      "metadata": {
        "id": "4JFvrZUZjfeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PTB Dataset\n",
        "X_train, Y_train = format_data(df_train_ptb, padded_size)\n",
        "X_test, Y_test   = format_data(df_test_ptb, padded_size)\n",
        "\n",
        "# Parameters to test\n",
        "# We adjust depth and with. We use power of two widths and depth of 1 or 2\n",
        "parameters = {\n",
        "    \"depth\": [1, 2],\n",
        "    \"width\": [32, 64, 128],\n",
        "}\n",
        "\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Set up callbacks for base model\n",
        "    file_path = model_dir.joinpath(\"latent_ptbdb.h5\")\n",
        "    checkpoint = ModelCheckpoint(file_path, monitor=\"val_acc\", verbose=1, save_best_only=True, mode='max')\n",
        "    early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=30, verbose=1)\n",
        "    redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=15, verbose=2)\n",
        "    callbacks_list = [checkpoint, early, redonplat]\n",
        "\n",
        "    # Set up base model\n",
        "    base = LatentClassifier(classes = 1, width=None, depth=None, encoder=model, \n",
        "                            epochs=1000, verbose=0, callbacks=callbacks_list, validation_split=0.1)\n",
        "    \n",
        "    # Run grid search\n",
        "    search = GridSearchCV(base, parameters, verbose=3, cv=2)\n",
        "    search.fit(X_train, Y_train)\n",
        "    print(f\"Finished CV for PTB Dataset: Top score {search.best_score_}\\n\"\n",
        "          f\"Best parameters: {search.cv_results_['params'][search.best_index_]}\")\n",
        "    \n",
        "    # Run tests\n",
        "    pred_test = search.best_estimator_.predict(X_test)\n",
        "\n",
        "    f1 = f1_score(Y_test, pred_test)\n",
        "    print(\"Test f1 score : %s \"% f1)\n",
        "\n",
        "    acc = accuracy_score(Y_test, pred_test)\n",
        "    print(\"Test accuracy score : %s \"% acc)\n",
        "\n",
        "    auroc = roc_auc_score(Y_test, pred_test)\n",
        "    print(\"Test AUROC : %s \"% auroc)\n",
        "\n",
        "    auprc = average_precision_score(Y_test, pred_test)\n",
        "    print(\"Test AUPRC : %s \"% auprc)\n"
      ],
      "metadata": {
        "id": "aDZ5XgQgVGZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on MIT-BIH Dataset"
      ],
      "metadata": {
        "id": "VYqPC9BSjqh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PTB Dataset\n",
        "X_train, Y_train = format_data(df_train_mit, padded_size)\n",
        "X_test, Y_test   = format_data(df_test_mit, padded_size)\n",
        "\n",
        "# Parameters to test\n",
        "# We adjust depth and with. We use power of two widths and depth of 1 or 2\n",
        "parameters = {\n",
        "    \"depth\": [1, 2],\n",
        "    \"width\": [32, 64, 128],\n",
        "}\n",
        "\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Set up callbacks for base model\n",
        "    file_path = model_dir.joinpath(\"latent_mit.h5\")\n",
        "    checkpoint = ModelCheckpoint(file_path, monitor=\"val_acc\", verbose=1, save_best_only=True, mode=\"max\")\n",
        "    early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=30, verbose=1)\n",
        "    redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=15, verbose=2)\n",
        "    callbacks_list = [checkpoint, early, redonplat]\n",
        "\n",
        "    # Set up base model\n",
        "    base = LatentClassifier(classes = 5, width=None, depth=None, encoder=model, \n",
        "                            epochs=1000, verbose=0, callbacks=callbacks_list, validation_split=0.1)\n",
        "    \n",
        "    # Run grid search\n",
        "    search = GridSearchCV(base, parameters, verbose=3, cv=2)\n",
        "    search.fit(X_train, Y_train)\n",
        "    print(f\"Finished CV for MIT-BIH Dataset: Top score {search.best_score_}\\n\"\n",
        "          f\"Best parameters: {search.cv_results_['params'][search.best_index_]}\")\n",
        "    \n",
        "    # Run tests\n",
        "    pred_test = search.best_estimator_.predict(X_test)\n",
        "\n",
        "    f1 = f1_score(Y_test, pred_test, average=\"macro\")\n",
        "    print(\"Test f1 score : %s \"% f1)\n",
        "\n",
        "    acc = accuracy_score(Y_test, pred_test)\n",
        "    print(\"Test accuracy score : %s \"% acc)\n"
      ],
      "metadata": {
        "id": "ms9LXtl4jpvr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}