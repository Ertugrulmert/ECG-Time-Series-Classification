{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d393bf0",
   "metadata": {},
   "source": [
    "# Task 4: Transfer Learning using ResNet\n",
    "\n",
    "In the following we implement a transfer learning approach for our ResNet model. We first trained the model on data from the [MIT-BIH Arrythmia Database](https://physionet.org/content/mitdb/1.0.0/) and then retrained some layers on [PTB Diagnostic ECG Database](https://physionet.org/physiobank/database/ptbdb/). \n",
    "\n",
    "We investiage three different transfer learning approaches. For each one of them the hyperparameters are tuned with a grid search. Then the final model is trained with all available training data and the classes for the test set are predicted. \n",
    "\n",
    "Further information can be found in the corresponding section of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras import losses, activations, models\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, average_precision_score\n",
    "from TransferResNet_Models import get_ResNet_frozenBase\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Input, Dropout, Conv1D, ReLU, add, MaxPool1D, Flatten, BatchNormalization\n",
    "from keras import Model\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_1 = pd.read_csv(\"../input/ptbdb_normal.csv\", header=None)\n",
    "df_2 = pd.read_csv(\"../input/ptbdb_abnormal.csv\", header=None)\n",
    "df = pd.concat([df_1, df_2])\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1337, stratify=df[187])\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82bdfc",
   "metadata": {},
   "source": [
    "## Approach 1: Frozen Base Model\n",
    "\n",
    "The residual blocks are kept non-trainable and kept with the weights trained on the MIT-BIH dataset. Only the fully connected layer will be trained on the PTBDB data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6679d",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28310ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_results = {}\n",
    "\n",
    "opts = [\"Adam\",\"rmsprop\"]\n",
    "learning_rates = [(\"const\", 0.0001), (\"const\", 0.0005), (\"const\", 0.001)]\n",
    "\n",
    "for opt in opts: \n",
    "    for mode,factor in learning_rates:\n",
    "        if mode == \"const\":\n",
    "            lr = factor\n",
    "        elif mode==\"exponential\": \n",
    "            lr = optimizers.schedules.ExponentialDecay(initial_learning_rate=0.005,decay_steps=1000,decay_rate=factor)\n",
    "        else:\n",
    "            lr = optimizers.schedules.PolynomialDecay(initial_learning_rate=0.01,decay_steps=1000,power=factor)\n",
    "        if opt== \"Adam\":\n",
    "            optimizer = optimizers.Adam(learning_rate = lr)\n",
    "        elif opt == \"SGD\":\n",
    "            optimizer = optimizers.SGD(learning_rate = lr)\n",
    "        elif opt == \"rmsprop\":\n",
    "            optimizer = optimizers.RMSprop(learning_rate = lr)\n",
    "\n",
    "        grid_search_results[f\"{opt}_{mode}_{factor}\"] = []\n",
    "\n",
    "        for fold, (train, val) in enumerate(KFold(n_splits=5, shuffle=True,random_state = 123).split(X,Y)):\n",
    "            print(f\"FOLD: {fold} OPT {opt} MODDE {mode} FACTOR {factor}\")\n",
    "            \n",
    "            resnet_model = ResNetStandard(5,0.1,optimizer=optimizers.RMSprop(0.001)).model_builder()\n",
    "            resnet_model.load_weights(\"Results/ResNet_MITBIH.h5\")\n",
    "\n",
    "            # freeze layers\n",
    "            for i, layer in enumerate(resnet_model.layers):\n",
    "                layer.trainable = False\n",
    "                if(i >= len(resnet_model.layers) - 5):\n",
    "                    break\n",
    "\n",
    "            # changing the new final fully connected layer\n",
    "            output = Dense(1, activation=activations.sigmoid)(resnet_model.layers[len(resnet_model.layers)-2].output)\n",
    "            transfered_model = Model(inputs=resnet_model.input, outputs=output)\n",
    "            transfered_model.compile(loss=losses.binary_crossentropy, optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "            # train model\n",
    "            file_path_frozen_base = f\"Results/Transfer_ResNetFrozenBase_Hyperparam.h5\"\n",
    "            checkpoint_frozen_base = ModelCheckpoint(file_path_frozen_base, monitor='val_acc', verbose=1, save_best_only=True, mode='max')    \n",
    "            early_frozen_base = EarlyStopping(monitor='val_acc', patience=7)\n",
    "            callbacks_list_frozen_base = [checkpoint_frozen_base, early_frozen_base] \n",
    "\n",
    "            transfered_model.fit(X[train], Y[train], epochs=100, batch_size=128, verbose=1, callbacks=callbacks_list_frozen_base, validation_data = (X[val],Y[val]))\n",
    "\n",
    "            grid_search_results[f\"{opt}_{mode}_{factor}\"].append(transfered_model.evaluate(X[val],Y[val]))\n",
    "\n",
    "            with open(\"Results/FrozenBase_Hyperparam.json\", \"w\") as outfile:\n",
    "                json.dump(grid_search_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Results/FrozenBase_Hyperparam.json')\n",
    "grid_search_results = json.load(f)\n",
    "means = []\n",
    "for k in grid_search_results.keys():\n",
    "    mean_acc = sum(elt for elt in grid_search_results[k])/len(grid_search_results[k])\n",
    "    means.append((k, mean_loss))\n",
    "\n",
    "means.sort(reverse=True)\n",
    "means[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc5527b",
   "metadata": {},
   "source": [
    "Based on the grid search the optimal parameters for the model are: \n",
    "\n",
    "optimizer: **Adam**\n",
    "\n",
    "learning rate: **constant of 0.001**\n",
    "\n",
    "These parameters obtained performances of (average loss,  average accuracy): \n",
    "\n",
    "**(0.07962191924452781, 0.9737127780914306)**\n",
    "\n",
    "### Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ae0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResNetStandard(5,0.1,optimizer=optimizers.RMSprop(0.001)).model_builder()\n",
    "resnet_model.load_weights(\"Results/ResNet_MITBIH.h5\")\n",
    "\n",
    "# freeze layers\n",
    "for i, layer in enumerate(resnet_model.layers):\n",
    "    layer.trainable = False\n",
    "    if(i >= len(resnet_model.layers) - 5):\n",
    "        break\n",
    "\n",
    "# changing the new final fully connected layer\n",
    "output = Dense(1, activation=activations.sigmoid)(resnet_model.layers[len(resnet_model.layers)-2].output)\n",
    "transfered_model = Model(inputs=resnet_model.input, outputs=output)\n",
    "transfered_model.compile(loss=losses.binary_crossentropy, optimizer=optimizers.Adam(0.001), metrics=['acc'])\n",
    "\n",
    "# train model\n",
    "file_path_frozen_base = f\"Results/Transfer_ResNetFrozenBase.h5\"\n",
    "checkpoint_frozen_base = ModelCheckpoint(file_path_frozen_base, monitor='val_acc', verbose=1, save_best_only=True, mode='max')    \n",
    "early_frozen_base = EarlyStopping(monitor='val_acc', patience=10)\n",
    "callbacks_list_frozen_base = [checkpoint_frozen_base, early_frozen_base] \n",
    "\n",
    "transfered_model.fit(X, Y, epochs=200, batch_size=128, verbose=1, callbacks=callbacks_list_frozen_base, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daaa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfered_model.load_weights(\"Results/Transfer_ResNetFrozenBase.h5\")\n",
    "pred_test = transfered_model.predict(X_test)\n",
    "pred_test = (pred_test>0.5).astype(np.int8)\n",
    "\n",
    "f1 = f1_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test accuracy : %s \"% acc)\n",
    "\n",
    "auroc = roc_auc_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test AUROC : %s \"% auroc)\n",
    "\n",
    "auprc = average_precision_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test AUPRC : %s \"% auprc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394ce5c",
   "metadata": {},
   "source": [
    "**Performances:**\n",
    "\n",
    "Test f1 score : 0.9742979533555449 \n",
    "\n",
    "Test accuracy : 0.9628993473033322 \n",
    "\n",
    "Test AUROC : 0.9541607322004235 \n",
    "\n",
    "Test AUPRC : 0.9681505678692455"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60534367",
   "metadata": {},
   "source": [
    "## Approach 2: Retrain whole model\n",
    "\n",
    "The last layer of the model is changed so it matches the PTBDB data. Then all weights are retrained with PTBDB data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852fca63",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a9479",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_results = {}\n",
    "\n",
    "opts = [\"Adam\",\"rmsprop\"]\n",
    "learning_rates = [(\"const\", 0.0001), (\"const\", 0.0005), (\"const\", 0.001)]\n",
    "\n",
    "for opt in opts: \n",
    "    for mode,factor in learning_rates:\n",
    "        if mode == \"const\":\n",
    "            lr = factor\n",
    "        elif mode==\"exponential\": \n",
    "            lr = optimizers.schedules.ExponentialDecay(initial_learning_rate=0.005,decay_steps=1000,decay_rate=factor)\n",
    "        else:\n",
    "            lr = optimizers.schedules.PolynomialDecay(initial_learning_rate=0.01,decay_steps=1000,power=factor)\n",
    "        if opt== \"Adam\":\n",
    "            optimizer = optimizers.Adam(learning_rate = lr)\n",
    "        elif opt == \"SGD\":\n",
    "            optimizer = optimizers.SGD(learning_rate = lr)\n",
    "        elif opt == \"rmsprop\":\n",
    "            optimizer = optimizers.RMSprop(learning_rate = lr)\n",
    "\n",
    "        grid_search_results[f\"{opt}_{mode}_{factor}\"] = []\n",
    "\n",
    "        for fold, (train, val) in enumerate(KFold(n_splits=5, shuffle=True,random_state = 123).split(X,Y)):\n",
    "            print(f\"FOLD: {fold} OPT {opt} MODDE {mode} FACTOR {factor}\")\n",
    "            \n",
    "            resnet_model = ResNetStandard(5,0.1,optimizer=optimizers.RMSprop(0.001)).model_builder()\n",
    "            resnet_model.load_weights(\"Results/ResNet_MITBIH.h5\")\n",
    "\n",
    "            # changing the new final fully connected layer\n",
    "            output = Dense(1, activation=activations.sigmoid)(resnet_model.layers[len(resnet_model.layers)-2].output)\n",
    "            transfered_model = Model(inputs=resnet_model.input, outputs=output)\n",
    "            transfered_model.compile(loss=losses.binary_crossentropy, optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "            # train model\n",
    "            file_path_fullretrain = f\"Results/Transfer_ResNetFullRetrain_Hyperparam.h5\"\n",
    "            checkpoint_fullretrain = ModelCheckpoint(file_path_fullretrain, monitor='val_acc', verbose=1, save_best_only=True, mode='max')    \n",
    "            early_fullretrain = EarlyStopping(monitor='val_acc', patience=7)\n",
    "            callbacks_list_fullretrain = [checkpoint_fullretrain, early_fullretrain] \n",
    "\n",
    "            transfered_model.fit(X[train], Y[train], epochs=1, batch_size=128, verbose=1, callbacks=callbacks_list_fullretrain, validation_data = (X[val],Y[val]))\n",
    "\n",
    "            grid_search_results[f\"{opt}_{mode}_{factor}\"].append(transfered_model.evaluate(X[val],Y[val]))\n",
    "\n",
    "            with open(\"Results/FullRetrain_Hyperparam.json\", \"w\") as outfile:\n",
    "                json.dump(grid_search_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ffb12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Results/FullRetrain_Hyperparam.json')\n",
    "grid_search_results = json.load(f)\n",
    "means = []\n",
    "for k in grid_search_results.keys():\n",
    "    mean_loss = sum(elt[0] for elt in grid_search_results[k])/len(grid_search_results[k])\n",
    "    mean_acc = sum(elt[1] for elt in grid_search_results[k])/len(grid_search_results[k])\n",
    "    means.append((k, mean_loss, mean_acc))\n",
    "\n",
    "means.sort(reverse=True)\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775067e",
   "metadata": {},
   "source": [
    "Based on the grid search the optimal parameters for the model are: \n",
    "\n",
    "optimizer: **Adam**\n",
    "\n",
    "learning rate: **constant of 0.001**\n",
    "\n",
    "These parameters obtained performances of (average loss,  average accuracy): \n",
    "\n",
    "**(0.06563280522823334, 0.9768921613693238)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da17156e",
   "metadata": {},
   "source": [
    "### Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a227cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResNetStandard(5,0.1,optimizer=optimizers.RMSprop(0.001)).model_builder()\n",
    "resnet_model.load_weights(\"Results/ResNet_MITBIH.h5\")\n",
    "\n",
    "# changing the new final fully connected layer\n",
    "output = Dense(1, activation=activations.sigmoid)(resnet_model.layers[len(resnet_model.layers)-2].output)\n",
    "transfered_model = Model(inputs=resnet_model.input, outputs=output)\n",
    "transfered_model.compile(loss=losses.binary_crossentropy, optimizer=optimizers.Adam(0.001), metrics=['acc'])\n",
    "\n",
    "# train model\n",
    "file_path_frozen_base = f\"Results/Transfer_ResNet_FullRetrain2.h5\"\n",
    "checkpoint_frozen_base = ModelCheckpoint(file_path_frozen_base, monitor='val_acc', verbose=1, save_best_only=True, mode='max')    \n",
    "early_frozen_base = EarlyStopping(monitor='val_acc', patience=10)\n",
    "callbacks_list_frozen_base = [checkpoint_frozen_base, early_frozen_base] \n",
    "\n",
    "transfered_model.fit(X, Y, epochs=200, batch_size=128, verbose=1, callbacks=callbacks_list_frozen_base, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6858de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfered_model.load_weights(\"Results/Transfer_ResNet_FullRetrain.h5\")\n",
    "pred_test = transfered_model.predict(X_test)\n",
    "pred_test = (pred_test>0.5).astype(np.int8)\n",
    "\n",
    "f1 = f1_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test accuracy : %s \"% acc)\n",
    "\n",
    "auroc = roc_auc_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test AUROC : %s \"% auroc)\n",
    "\n",
    "auprc = average_precision_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test AUPRC : %s \"% auprc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87045bdd",
   "metadata": {},
   "source": [
    "**Performances**:\n",
    "\n",
    "Test f1 score : 0.9825389334591789\n",
    "\n",
    "Test accuracy : 0.9745791824115424\n",
    "\n",
    "Test AUROC : 0.9618680896056379 \n",
    "\n",
    "Test AUPRC : 0.9723153858829998 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c07682",
   "metadata": {},
   "source": [
    "## Approach 3: first frozen base model, then re-training whole model.\n",
    "\n",
    "The last layer of the model is changed so it matches the PTBDB data. Then the fully connected layer is retrained with the PTBDB data. After convergence, all weights are retrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_model = get_ResNet_frozenBase(5, 0.1)\n",
    "frozen_model.load_weights(\"Results/ResNet_MITBIH_PTBDB_params.h5\")\n",
    "number_layers = len(frozen_model.layers)\n",
    "\n",
    "# changing the new final fully connected layer\n",
    "output = Dense(1, activation=activations.sigmoid)(frozen_model.layers[number_layers-2].output)\n",
    "new_model = Model(inputs=frozen_model.input, outputs=output)\n",
    "new_model.compile(loss=losses.binary_crossentropy, optimizer=optimizers.RMSprop(0.001), metrics=['acc'])\n",
    "new_model.summary()\n",
    "\n",
    "# train model\n",
    "file_path_frozen_base = f\"Results/Transfer_ResNet_FrozenBaseRetrainWhole.h5\"\n",
    "checkpoint_frozen_base = ModelCheckpoint(file_path_frozen_base, monitor='val_acc', verbose=1, save_best_only=True, mode='max')    \n",
    "early_frozen_base = EarlyStopping(monitor='val_acc', patience=7)\n",
    "callbacks_list_frozen_base = [checkpoint_frozen_base, early_frozen_base] \n",
    "\n",
    "new_model.fit(X, Y, epochs=200, batch_size=128, verbose=2, callbacks=callbacks_list_frozen_base, validation_split=0.1)\n",
    "\n",
    "for layer in new_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "new_model.compile(loss=losses.binary_crossentropy, optimizer=optimizers.RMSprop(0.0005), metrics=['acc'])\n",
    "\n",
    "new_model.fit(X, Y, epochs=200, batch_size=128, verbose=2, callbacks=callbacks_list_frozen_base, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ae4cb",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_results = {}\n",
    "\n",
    "#opts = [\"Adam\",\"rmsprop\"]\n",
    "opts = [\"rmsprop\"]\n",
    "learning_rates = [(\"const\", 0.0001), (\"const\", 0.0005), (\"const\", 0.001)]\n",
    "\n",
    "for opt in opts: \n",
    "    for mode,factor in learning_rates:\n",
    "        for mode2,factor2 in learning_rates:\n",
    "            if mode == \"const\":\n",
    "                lr = factor\n",
    "            elif mode==\"exponential\": \n",
    "                lr = optimizers.schedules.ExponentialDecay(initial_learning_rate=0.005,decay_steps=1000,decay_rate=factor)\n",
    "            else:\n",
    "                lr = optimizers.schedules.PolynomialDecay(initial_learning_rate=0.01,decay_steps=1000,power=factor)\n",
    "            if opt== \"Adam\":\n",
    "                optimizer = optimizers.Adam(learning_rate = lr)\n",
    "                optimizer2 = optimizers.Adam(learning_rate = factor2)\n",
    "            elif opt == \"SGD\":\n",
    "                optimizer = optimizers.SGD(learning_rate = lr)\n",
    "            elif opt == \"rmsprop\":\n",
    "                optimizer = optimizers.RMSprop(learning_rate = lr)\n",
    "                optimizer2 = optimizers.RMSprop(learning_rate = factor2)\n",
    "\n",
    "            grid_search_results[f\"{opt}_{mode}_{factor}_{mode2}_{factor2}\"] = []\n",
    "\n",
    "            for fold, (train, val) in enumerate(KFold(n_splits=5, shuffle=True,random_state = 123).split(X,Y)):\n",
    "                print(f\"FOLD: {fold} OPT {opt} MODDE {mode} FACTOR {factor}\")\n",
    "                \n",
    "                \n",
    "                resnet_model = ResNetStandard(5,0.1,optimizer=optimizers.RMSprop(0.001)).model_builder()\n",
    "                resnet_model.load_weights(\"Results/ResNet_MITBIH.h5\")\n",
    "                \n",
    "                # freeze layers\n",
    "                for i, layer in enumerate(resnet_model.layers):\n",
    "                    layer.trainable = False\n",
    "                    if(i >= len(resnet_model.layers) - 5):\n",
    "                        break\n",
    "\n",
    "                # changing the new final fully connected layer\n",
    "                output = Dense(1, activation=activations.sigmoid)(resnet_model.layers[len(resnet_model.layers)-2].output)\n",
    "                transfered_model = Model(inputs=resnet_model.input, outputs=output)\n",
    "                transfered_model.compile(loss=losses.binary_crossentropy, optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "                # train model\n",
    "                file_path_frozen_retrain = f\"Results/Transfer_FrozenBaseRetrainWhole_Hyperparam.h5\"\n",
    "                checkpoint_frozen_retrain = ModelCheckpoint(file_path_frozen_retrain, monitor='val_acc', verbose=1, save_best_only=True, mode='max')    \n",
    "                early_frozen_retrain = EarlyStopping(monitor='val_acc', patience=7)\n",
    "                callbacks_list_frozen_retrain= [checkpoint_frozen_retrain, early_frozen_retrain] \n",
    "                \n",
    "                transfered_model.fit(X, Y, epochs=200, batch_size=128, verbose=1, callbacks=callbacks_list_frozen_retrain, validation_data = (X[val],Y[val]))\n",
    "\n",
    "                \n",
    "                for layer in transfered_model.layers:\n",
    "                    layer.trainable = True\n",
    "                \n",
    "                transfered_model.compile(loss=losses.binary_crossentropy, optimizer=optimizer2, metrics=['acc'])\n",
    "                transfered_model.fit(X, Y, epochs=200, batch_size=128, verbose=1, callbacks=callbacks_list_frozen_retrain, validation_data = (X[val],Y[val]))\n",
    "\n",
    "                grid_search_results[f\"{opt}_{mode}_{factor}_{mode2}_{factor2}\"].append(transfered_model.evaluate(X[val],Y[val], verbose = 0))\n",
    "\n",
    "                with open(\"Results/FrozenBaseRetrainWhole_Hyperparam_RMSPROP.json\", \"w\") as outfile:\n",
    "                    json.dump(grid_search_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Results/FrozenBaseRetrainWhole_Hyperparam.json')\n",
    "grid_search_results = json.load(f)\n",
    "means = []\n",
    "for k in grid_search_results.keys():\n",
    "    mean_loss = sum(elt[0] for elt in grid_search_results[k])/len(grid_search_results[k])\n",
    "    mean_acc = sum(elt[1] for elt in grid_search_results[k])/len(grid_search_results[k])\n",
    "    means.append((k, mean_loss, mean_acc))\n",
    "\n",
    "means.sort(reverse=True)\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf491e2",
   "metadata": {},
   "source": [
    "Based on the grid search the optimal parameters for the model are: \n",
    "\n",
    "optimizer1: **Adam**       learning rate1: **constant of 0.0005**\n",
    "\n",
    "optimizer2: **Adam**       learning rate1: **constant of 0.0005**\n",
    "\n",
    "These parameters obtained performances of (average loss,  average accuracy): \n",
    "\n",
    "**(0.00659539841581136, 0.9987114071846008)**\n",
    "\n",
    "('rmsprop_const_0.001_const_0.0005',\n",
    "  0.006532190646976232,\n",
    "  0.9986255288124084),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d459010",
   "metadata": {},
   "source": [
    "## Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf33886",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResNetStandard(5,0.1,optimizer=optimizers.RMSprop(0.001)).model_builder()\n",
    "resnet_model.load_weights(\"Results/ResNet_MITBIH.h5\")\n",
    "\n",
    "# freeze layers\n",
    "for i, layer in enumerate(resnet_model.layers):\n",
    "    layer.trainable = False\n",
    "    if(i >= len(resnet_model.layers) - 5):\n",
    "        break\n",
    "\n",
    "# changing the new final fully connected layer\n",
    "output = Dense(1, activation=activations.sigmoid)(resnet_model.layers[len(resnet_model.layers)-2].output)\n",
    "transfered_model = Model(inputs=resnet_model.input, outputs=output)\n",
    "transfered_model.compile(loss=losses.binary_crossentropy, optimizer=optimizers.Adam(0.0005), metrics=['acc'])\n",
    "\n",
    "# train model\n",
    "file_path_frozen_retrain = f\"Results/Transfer_ResNet_FrozenBaseRetrainWhole.h5\"\n",
    "checkpoint_frozen_retrain = ModelCheckpoint(file_path_frozen_retrain, monitor='val_acc', verbose=1, save_best_only=True, mode='max')    \n",
    "early_frozen_retrain = EarlyStopping(monitor='val_acc', patience=10)\n",
    "callbacks_list_frozen_retrain= [checkpoint_frozen_retrain, early_frozen_retrain] \n",
    "\n",
    "transfered_model.fit(X, Y, epochs=200, batch_size=128, verbose=1, callbacks=callbacks_list_frozen_retrain, validation_data = (X[val],Y[val]))\n",
    "\n",
    "\n",
    "for layer in transfered_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "transfered_model.compile(loss=losses.binary_crossentropy, optimizer=optimizers.Adam(0.0005), metrics=['acc'])\n",
    "transfered_model.fit(X, Y, epochs=200, batch_size=128, verbose=1, callbacks=callbacks_list_frozen_retrain, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c827a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfered_model.load_weights(\"Results/Transfer_ResNet_FrozenBaseRetrainWhole.h5\")\n",
    "pred_test = transfered_model.predict(X_test)\n",
    "pred_test = (pred_test>0.5).astype(np.int8)\n",
    "\n",
    "f1 = f1_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test accuracy : %s \"% acc)\n",
    "\n",
    "auroc = roc_auc_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test AUROC : %s \"% auroc)\n",
    "\n",
    "auprc = average_precision_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test AUPRC : %s \"% auprc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e7705",
   "metadata": {},
   "source": [
    "**Performances:**\n",
    "\n",
    "Test f1 score : 0.9961977186311787 \n",
    "\n",
    "Test accuracy : 0.9945036070079011 \n",
    "\n",
    "Test AUROC : 0.9923923181054244 \n",
    "\n",
    "Test AUPRC : 0.9944719387044904 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4HC1",
   "language": "python",
   "name": "ml4hc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
